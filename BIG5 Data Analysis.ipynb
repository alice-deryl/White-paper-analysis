{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from __future__ import print_function\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from datetime import datetime\n",
    "import csv\n",
    "\n",
    "from statsmodels import *\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import *\n",
    "\n",
    "from sklearn import linear_model\n",
    "import matplotlib as mpl\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Import survey responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Survey Responses from Google Sheet\n",
    "#Procedure to retrieve latest survey responses.\n",
    "#Establishes connection to google API using json key stored in local folder\n",
    "#Downloads latest responses in the google sheets\n",
    "#Returns pandas dataframe with all reponses. \n",
    "\n",
    "#Specify sheet name\n",
    "spreadsheet = '2020-08-14 Matt Matthew Walford BIG5 Psycometric Survey Free personality report from your online public profiles responses to forms survey https://docs.google.com/forms/d/e/1FAIpQLSd3hm7vkXpIaHg4KTzLriyxk71ec2qbSdgkV7beLOmSQIOszA/viewform shared with Vei Yie and alice alice.d.matthews@gmail.com'\n",
    "    \n",
    "#Specify Json key location\n",
    "json_file = 'survey-personality-71154dfbe30a.json'\n",
    "    \n",
    "#load in json file key\n",
    "json_key = json.load(open(json_file))\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(json_file)\n",
    "        \n",
    "# Find a workbook by name and open the first sheet\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "#Open Survey Data of the spreadsheet and intalise as a variable\n",
    "survey_sheet = client.open(spreadsheet).sheet1\n",
    "\n",
    "#Convert sheet to a pandas dataframe\n",
    "survey_data = pd.DataFrame(survey_sheet.get_all_records())\n",
    "\n",
    "# set the index to match the Google Sheet index.  Important because the index functions as a unique ID for each respondent\n",
    "index = pd.Index(range(2, len(survey_data)+2))\n",
    "survey_data.set_index(index, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Demographic Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of ages among survey respondents\n",
    "\n",
    "age_data = survey_data[\"What age bracket do you fall in?\"]\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(10, 6)}, font_scale = 1.2)\n",
    "ax = sns.countplot(age_data, order = ['18-24', '25-29', '30-34', '35-39', '40-44', '45-49', '50-54', '55-59', '60-64', '65+', 'Prefer not to say'])\n",
    "ax.set_title(label = f\"Age distribution among survey respondents\", fontsize = 18, loc = 'left')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of countries among respondents\n",
    "\n",
    "country_data = survey_data[\"What is your country of residence?\"]\n",
    "sns.set(rc = {'figure.figsize':(15,8)}, font_scale = 1.2)\n",
    "ax = sns.countplot(country_data,\n",
    "                    order = country_data.value_counts().index)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
    "ax.set_title(label = 'Country distribution among survey respondents', fontsize = 18, loc = 'left')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender distribution of survey respondents\n",
    "gender_data = survey_data['What is your sex?']\n",
    "\n",
    "sns.set(rc = {'figure.figsize':(10,8)}, font_scale = 1.2)\n",
    "ax = sns.countplot(gender_data, palette = \"husl\")\n",
    "ax.set_title(label = 'Gender distribution among survey respondents', fontsize = 18, loc = 'left')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex gender data to make the pie plot display nicely\n",
    "gender_data = gender_data.value_counts().reindex(['Male', 'Prefer not to say', 'Female', 'Non-binary'])\n",
    "\n",
    "colours = sns.color_palette(\"Set2\")\n",
    "\n",
    "fig = plt.pie(gender_data,\n",
    "        labels=['Male', 'Prefer not to say', 'Female', 'Non-binary'],\n",
    "        startangle=45,\n",
    "        autopct='%1.1f%%',\n",
    "        textprops={'fontsize': 15},\n",
    "        colors = colours)\n",
    "\n",
    "plt.title('Gender distribution among survey respondents', fontsize=18, loc = 'center')\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Calculate IPIP 50-item Big 5 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the CSV file of the Big 5 Survey questions, description and scoring code\n",
    "survey_code = pd.read_csv('Big 5 Survey Code.csv')\n",
    "survey_code\n",
    "question_codes = []\n",
    "x = 1\n",
    "for i in list(survey_code.Code):\n",
    "    q = f\"Q{x} {i}\"\n",
    "    question_codes.append(q)\n",
    "    x = x + 1\n",
    "    \n",
    "# add useful column names and delete unneccessary columns\n",
    "col_names = question_codes\n",
    "for x in ['LinkedIn', 'Reddit', 'Twitter', 'Stacko', 'Gender', 'Age', 'Country']:\n",
    "    col_names.append(x)\n",
    "columns_to_drop = [0, 58, 59, 60, 61, 62, 63, 64, 65] # drop the timestamp and unnecessary details\n",
    "survey_data.drop(survey_data.columns[columns_to_drop], axis = 1, inplace = True)\n",
    "survey_data.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to the appropriate numeric scorings\n",
    "# questions with 'NEG' in the tag are negatively scored, and the final score is reversed from 6 so that 5 becomes 1, 4 becomes 2 etc\n",
    "survey_numerated = survey_data.replace({\"Very Accurate\": 5, \"Moderately Accurate\": 4, \"Neither Accurate Nor Inaccurate\": 3, \"Moderately Inaccurate\": 2, \"Very Inaccurate\": 1})\n",
    "\n",
    "for col in survey_numerated.columns:\n",
    "    if 'NEG' in col:\n",
    "        survey_numerated[col] = 6-survey_numerated[col]\n",
    "\n",
    "survey_numerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate scores as a decimal\n",
    "survey_numerated['survey_openness_raw'] = (survey_numerated.filter(regex='OPE', axis = 1).mean(axis = 1))/5 \n",
    "survey_numerated['survey_conscientiousness_raw'] = (survey_numerated.filter(regex='CON', axis = 1).mean(axis = 1))/5\n",
    "survey_numerated['survey_extraversion_raw'] = (survey_numerated.filter(regex='EXT', axis = 1).mean(axis = 1))/5\n",
    "survey_numerated['survey_agreeableness_raw'] = (survey_numerated.filter(regex='AGR', axis = 1).mean(axis = 1))/5\n",
    "survey_numerated['survey_emotional_stability_raw'] = (survey_numerated.filter(regex='EMO', axis = 1).mean(axis = 1))/5\n",
    "\n",
    "survey_scores = survey_numerated[['survey_openness_raw', 'survey_conscientiousness_raw', 'survey_extraversion_raw','survey_agreeableness_raw', 'survey_emotional_stability_raw']].copy()\n",
    "survey_scores = survey_scores[~survey_scores.index.duplicated(keep = 'first')] #remove dupilicate indices.  The ~ is a 'not' operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Distributions of IPIP 50-item scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_survey_scores = survey_scores.copy() # make a copy with neater column names\n",
    "\n",
    "plot_survey_scores.columns = [\"Openness\", \"Conscientiousness\", \"Extraversion\", \"Agreeableness\", \"Emotional Stability\"]\n",
    "\n",
    "grid = sns.PairGrid(plot_survey_scores, diag_sharey=False)\n",
    "grid.map_lower(sns.scatterplot)\n",
    "grid.map_diag(sns.histplot)\n",
    "grid.set(xlim = (0,1), ylim = (0, 1))\n",
    "\n",
    "grid.fig.suptitle(\"Distributions of psychometric survey-based Big 5 trait scores, n = 367\", x = 0, y = 1.01, fontsize = 18, ha = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and add percentile scores\n",
    "\n",
    "survey_scores['survey_openness_percentile'] = survey_scores['survey_openness_raw'].rank(pct=True)\n",
    "survey_scores['survey_conscientiousness_percentile'] = survey_scores['survey_conscientiousness_raw'].rank(pct=True)\n",
    "survey_scores['survey_extraversion_percentile'] = survey_scores['survey_extraversion_raw'].rank(pct=True)\n",
    "survey_scores['survey_agreeableness_percentile'] = survey_scores['survey_agreeableness_raw'].rank(pct=True)\n",
    "survey_scores['survey_emotional_stability_percentile'] = survey_scores['survey_emotional_stability_raw'].rank(pct=True)\n",
    "\n",
    "\n",
    "survey_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5 = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'emotional_stability']\n",
    "for i in big_5:\n",
    "    plt.figure()\n",
    "    sns.set_theme(style = 'ticks')\n",
    "    sns.set(rc = {'figure.figsize':(10,8)})\n",
    "    ax = sns.histplot(survey_scores[f\"survey_{i}_raw\"])\n",
    "    ax.set_xlabel(xlabel = f\"survey {i} score\")\n",
    "    ax.set_xlim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Get Personality Insights Big 5 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Profile Data sheet and initialise as a variable\n",
    "profiles = client.open(spreadsheet).get_worksheet(1)\n",
    "profile_data = pd.DataFrame(profiles.get_all_records())\n",
    "\n",
    "# set the index to be the user_id column (which is the index of the survey sheet)\n",
    "profile_data.set_index(profile_data.user_id, drop = True, inplace=True)\n",
    "profile_data.sort_index(axis = 0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Personality Insights NLP scores from the profile_data df\n",
    "NLP_scores = profile_data.iloc[:, 11:28]\n",
    "NLP_scores = NLP_scores[~NLP_scores.index.duplicated(keep = 'first')] # drop duplicate indices\n",
    "\n",
    "NLP_scores.drop([i for i in NLP_scores.index if NLP_scores.openness_percentile[i] == ''], axis = 0, inplace=True) # drop empty rows\n",
    "\n",
    "NLP_scores[[i for i in NLP_scores.columns if 'percentile' in i]] \\\n",
    "    = NLP_scores[[i for i in NLP_scores.columns if 'percentile' in i]].replace('[\\%,]', '', regex=True).astype(float)/100 # drop the '%' sign, convert to float\n",
    "\n",
    "NLP_scores[[i for i in NLP_scores.columns if 'raw' in i]]\\\n",
    "    = NLP_scores[[i for i in NLP_scores.columns if 'raw' in i]].astype(float) # convert raw scores to floats\n",
    "\n",
    "\n",
    "# Append 'NLP_' to the NLP score column headers\n",
    "for i in big_5:\n",
    "    NLP_scores.rename(columns={f\"{i}_percentile\":f\"NLP_{i}_percentile\"}, inplace=True) \n",
    "    NLP_scores.rename(columns = {f\"{i}_raw\": f\"NLP_{i}_raw\"}, inplace = True)\n",
    "    NLP_scores.rename(columns = {f\"{i}_interpretation\": f\"NLP_{i}_interpretation\"}, inplace = True)\n",
    "    NLP_scores.rename(columns = {f\"{i}_my_percentile\": f\"NLP_{i}_my_percentile\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Distributions of Personality Insights scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_5 = ['openness', 'conscientiousness', 'extraversion', 'agreeableness', 'emotional_stability']\n",
    "\n",
    "for i in big_5:\n",
    "    plt.figure()\n",
    "    plt.xlim([0, 1])\n",
    "    sns.set_theme(style = 'ticks')\n",
    "    sns.set(rc = {'figure.figsize':(10,8)})\n",
    "    ax = sns.histplot(NLP_scores[f\"NLP_{i}_raw\"])\n",
    "    ax.set(xlabel = f\"{i.capitalize()} Score\")\n",
    "    ax.set_title(label = 'Distribution of online content scores', fontsize = 18, loc = 'left')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Add all scores to a single df for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = pd.concat([survey_scores, NLP_scores], axis=1, join=\"inner\")\n",
    "\n",
    "# create 3000+word and 1000+word dfs\n",
    "all_scores_3000 = all_scores[all_scores.total_word_count_passed > 2999].drop('total_word_count_passed', axis = 1).copy() \n",
    "all_scores_1000 = all_scores[all_scores.total_word_count_passed > 999].drop('total_word_count_passed', axis = 1).copy()\n",
    "\n",
    "# split on raw/percentile scores\n",
    "all_scores_3000_raw = all_scores_3000[[i for i in all_scores.columns if 'raw' in i]]\n",
    "all_scores_3000_percentiles = all_scores_3000[[i for i in all_scores.columns if 'percentile' in i]]\n",
    "\n",
    "all_scores_1000_raw = all_scores_1000[[i for i in all_scores.columns if 'raw' in i]]\n",
    "all_scores_1000_percentiles = all_scores_1000[[i for i in all_scores.columns if 'percentile' in i]]\n",
    "\n",
    "\n",
    "all_scores_3000_raw.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the data for analysis to 1000+/3000+ words, and raw/percentile\n",
    "\n",
    "data = all_scores_3000_raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "all_scores_3000_percentiles.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a correlation matrix heatmap\n",
    "corr_matrix_3000_raw = all_scores_3000_raw.corr(method ='pearson')\n",
    "corr_matrix_1000_raw = all_scores_1000_raw.corr(method = 'pearson')\n",
    "corr_matrix_3000_percentile = all_scores_3000_percentiles.corr(method = 'spearman')\n",
    "corr_matrix_1000_percentile = all_scores_1000_percentiles.corr(method = 'spearman')\n",
    "# create a mask to remove the upper triangle of the heatmap\n",
    "mask = np.triu(np.ones_like(corr_matrix_3000_raw, dtype=bool))\n",
    "\n",
    "for i in range(len(mask)): # make the diagonal 1s show in the final graph\n",
    "    mask[i][i] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a correlation matrix for 3000-word respondents\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# plot heatmap\n",
    "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr_matrix_3000_raw, mask=mask, annot=True, fmt=\".2f\", cmap=cmap,\n",
    "           vmin=-1, vmax=1, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "title = 'CORRELATION MATRIX - 3000 w of content\\nmethod: Pearson\\n'\n",
    "plt.title(title, loc='left', fontsize=18)\n",
    "\n",
    "# plot a correlation matrix for 3000-word percentiles with Spearman's Rho\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# plot heatmap\n",
    "cmap = sns.diverging_palette(0, 230, 90, 60, as_cmap=True)\n",
    "\n",
    "sns.heatmap(corr_matrix_3000_percentile, mask=mask, annot=True, fmt=\".2f\", cmap=cmap,\n",
    "           vmin=-1, vmax=1, cbar_kws={\"shrink\": .8})\n",
    "\n",
    "title = f\"CORRELATION MATRIX - 3000 w of content\\nmethod: Spearman's rho\"\n",
    "plt.title(title, loc='left', fontsize=18)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = all_scores_1000_percentiles\n",
    "print(f\"n (all_scores_1000) = {len(all_scores_1000_percentiles)}\")\n",
    "print(f\"n (all_scores_3000) = {len(all_scores_3000_percentiles)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in big_5:\n",
    "    plt.figure()\n",
    "    sns.set_theme(style = 'ticks')\n",
    "\n",
    "    rho, p = spearmanr(data[f\"survey_{i}_percentile\"], data[f\"NLP_{i}_percentile\"])\n",
    "    if p < 0.001:\n",
    "        p = 'P < 0.001'\n",
    "\n",
    "    elif p < 0.01:\n",
    "        p = 'P < 0.01'\n",
    "    \n",
    "    elif p < 0.05:\n",
    "        p = 'P < 0.05'\n",
    "\n",
    "    else:\n",
    "        p = f\"P > 0.05\"\n",
    "\n",
    "\n",
    "    sns.set(rc = {'figure.figsize':(10,8)})\n",
    "    ax = sns.regplot(x = f\"NLP_{i}_percentile\", y = f\"survey_{i}_percentile\", data = data)\n",
    "    ax.set(xlabel = f\"Online Content\", ylabel = f\"IPIP 50-item\")\n",
    "    ax.set_title(label = f\"Online Content vs. IPIP 50-item {i.capitalize()} Percentiles \\nSpearman's Rho = {round(rho, 2)}, {p}\", fontsize = 18, loc = 'left')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a dataset for analysis, add back the demographic variables\n",
    "regression_data = pd.concat([all_scores_1000_raw, survey_data[['Age', 'Country', 'Gender']]], axis = 1, join = 'inner').copy()\n",
    "\n",
    "# convert categorical data to dummy variables for regression\n",
    "regression_data = pd.get_dummies(regression_data, columns = ['Age', 'Country', 'Gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple regression (with demographic data as independent variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regressions on each Big 5 trait with all demographic data\n",
    "# this 'for loop' iterates through each big_5 trait and performs a regression on each, with demographic data\n",
    "for i in big_5:\n",
    "    X = regression_data[[f\"NLP_{i}_raw\", 'Age_18-24', 'Age_25-29', 'Age_30-34', 'Age_35-39', 'Age_40-44',\n",
    "       'Age_45-49', 'Age_50-54', 'Age_55-59', 'Age_65+','Country_Argentina',\n",
    "       'Country_Australia', 'Country_India', 'Country_Nigeria',\n",
    "       'Country_Philippines', 'Country_Qatar', 'Country_Romania',\n",
    "       'Country_Switzerland', 'Country_United Arab Emirates',\n",
    "       'Country_United Kingdom', 'Country_United States','Gender_Female',\n",
    "       'Gender_Male', 'Gender_Prefer not to say']]\n",
    "    y = regression_data[f\"survey_{i}_raw\"]\n",
    "\n",
    "    # X = sm.add_constant(X)\n",
    "\n",
    "    # Note the difference in argument order\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    pred_ols = model.get_prediction()\n",
    "    \n",
    "    iv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\n",
    "    iv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n",
    "\n",
    "    \n",
    "    # Print out the statistics\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple regression (Survey Data ~ NLP Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this for loop iterates through the Big 5 traits and performs a simple regression on each.\n",
    "# the independent variable (X) is the set of NLP scores for each trait\n",
    "for i in big_5:\n",
    "    X = regression_data[f\"NLP_{i}_raw\"]\n",
    "    y = regression_data[f\"survey_{i}_raw\"]\n",
    "\n",
    "    # Note the difference in argument order\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    pred_ols = model.get_prediction()\n",
    "    \n",
    "    iv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\n",
    "    iv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n",
    "\n",
    "    \n",
    "    # Print out the statistics\n",
    "    print(model.summary())\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9cf0e90108e5df5470aa6febe427bfefed043d9bad03cde02fff93fab4ad1bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Reputationaire': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
